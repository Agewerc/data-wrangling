{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2 - Text Pre-Processing & Feature Generation\n",
    "\n",
    "###  Group 103:\n",
    "\n",
    "##### Student Name: Alan Gewerc\n",
    "##### Student ID: 29961246\n",
    "##### Student Name: Cristiana Garcia Gewerc\n",
    "##### Student ID: 30088887\n",
    "\n",
    "\n",
    "Date: 14/09/2019\n",
    "\n",
    "Environment: Python 3.7.1 and Jupyter Notebook 5.7.4 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* pandas 0.23.4 (for data frame, included in Anaconda Python 3.7.1) \n",
    "* re 0.23.4 (for regular expression, included in Anaconda Python 3.7.1) \n",
    "* requests 2.21.0 (for getting data from url, not included in Anaconda)\n",
    "* pdfminer.pdfinterp (functions PDFResourceManager, PDFPageInterpreter)\n",
    "* pdfminer.converter (functions HTMLConverter,TextConverter,XMLConverter) \n",
    "* pdfminer.layout (function LAParams)\n",
    "* pdfminer.pdfpage (function PDFPage)\n",
    "* nltk.data (to load the tokenizer)\n",
    "* nltk.tokenize (functions RegexpTokenizer, MWETokenizer)\n",
    "* nltk.stem (function PorterStemmer)\n",
    "* nltk.util (function ngrams)\n",
    "* nltk.probability (functions such as FreqDist)\n",
    "* itertools (function chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This assignment comprises the execution of different text processing and analysis tasks applied to two hundred academic papers published in a popular AI conference. After extracting all data from the documents that are in a non-structured format, i.e., PDF's, preprocessing tasks are done (such as lowercase normalization and stemming) and finally the papers are converted to numerical representations (which are suitable inputs for NLP AI systems, etc). The required tasks, using python code, are the following:\n",
    "\n",
    "1. **PDF Extraction**: The document paper-ids.pdf contains a table in which each row contains a paper unique id and a URL. First, we have downloaded this file, extracted the information of the paper IDs and URLs from it. Them, we read the two hundred PDFs from these URLs and convert their contents into strings, that will further populate dictonaries containing all abstracts, titles, authors and bodies from the papers. \n",
    "2. **Sparse Feature Generation**: Focusing exclusively on the bodies dictonary, we generate two files: `vocab` and `count_vectors`. The first contains an index for every word or collocation from the data-set and the second the count of each index for every paper. However, firstly, some preprocessing tasks were done (the chosen order we will be explained throughout the assignment): \n",
    "    1. Normalization/segmentation\n",
    "    2. Tokenization\n",
    "    3. Identifying relevant bigrams\n",
    "    4. Removing\n",
    "        - independent stopwords\n",
    "        - dependent stopwords\n",
    "        - rare tokens\n",
    "        - small tokens (length less than 3)\n",
    "    8. Stemming\n",
    " \n",
    "3. **Statistics Generation**: Generate a dataframe with three columns containing the top10_terms_in_abstracts, top10_terms_in_titles, top10_authors after some preprocessing tasks were developed in the other title, abstract and author dictonaries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PDF Extraction\n",
    "\n",
    "Following the guidelines provided by the assessment document, we are using the `requests` package to programmatically download the PDF files and also `pdfminer` and `re` packages in order to read the PDF files into text and extract the required entities to complete the tasks. Aditionally, the `io` package will be used to handle unicodes with StringIO and BytesIO to use an in-memory buffer instead of a file.\n",
    "\n",
    "\n",
    "## 2.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "import re\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import HTMLConverter,TextConverter,XMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Read the PDFs\n",
    "\n",
    "First, we need to read the `paper-ids.pdf` file to extract the pdf links from it.\n",
    "\n",
    "We followed the approach [suggested](https://stackoverflow.com/questions/39854841/pdfminer-python-3-5?fbclid=IwAR0btjjjuzFet2zfp4Rhle3IG-ZOKP0iAAeToU7ewI7ly1-BLKcrS0MGDB8) by Haseeb (2018), using an adaptation of his `convert_pdf_to_txt` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have pdfminer installed and are ready to convert our PDF to text by running the following command:\n",
    "def convert_pdf_to_txt(path_to_file):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path_to_file, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use the above function to extract the text from the pdf file and save it into the string `links`. Them, we split it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename',\n",
       " 'url',\n",
       " 'PP1861.pdf',\n",
       " 'https://drive.google.com/uc?export=download&id=18BfdwBdmTd7DkE1LJUPNfTJifXzPToLU',\n",
       " 'PP3203.pdf',\n",
       " 'https://drive.google.com/uc?export=download&id=12IaCmFfJ7lAG7JIR0bEa-RVkgNrDmoZb',\n",
       " 'PP3216.pdf',\n",
       " 'https://drive.google.com/uc?export=download&id=18r6FpSWv6lkiHdDNfaTjssrXcvGUdyqO',\n",
       " 'PP3252.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading and converting the pdf file \n",
    "links = convert_pdf_to_txt(\"data/paper-ids.pdf\")\n",
    "# breaking it into lists items:\n",
    "links_list =links.split()\n",
    "# analyse the content:\n",
    "links_list[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the following pattern: every time a new page of the pdf file begins, a 'filename' and 'url' elements appears in our list. Those are useless for our purposes here. We are interested in keeping the filename associated with it's corresponding link.\n",
    "\n",
    "The filename is always ends with `.pdf`, and the associated url is the subsequent list element. We are going to store them in a dictionary called `links_dic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key PP1861, value https://drive.google.com/uc?export=download&id=18BfdwBdmTd7DkE1LJUPNfTJifXzPToLU \n",
      "key PP3203, value https://drive.google.com/uc?export=download&id=12IaCmFfJ7lAG7JIR0bEa-RVkgNrDmoZb \n",
      "key PP3216, value https://drive.google.com/uc?export=download&id=18r6FpSWv6lkiHdDNfaTjssrXcvGUdyqO \n"
     ]
    }
   ],
   "source": [
    "links_dic = {}\n",
    "# for i in range(1,20):\n",
    "for i in range(len(links_list)):\n",
    "    if links_list[i].endswith('.pdf'):\n",
    "        links_dic[re.sub('.pdf', \"\",links_list[i])] = links_list[i+1]\n",
    "# checking if the dictionary is working as it is suposed to:\n",
    "for x in list(links_dic)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  links_dic[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As [shared](https://stackoverflow.com/questions/22800100/parsing-a-pdf-via-url-with-python-using-pdfminer) by Haseeb (2018), we can use `pdf_from_url_to_txt` function to return a string from a pdf in a url without downloading it. I did a minor change in his function, using the `request` library instead of the `urllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_from_url_to_txt(url):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    f = requests.get(url).content\n",
    "    #f = urllib.urlopen(url).read()\n",
    "    fp = io.BytesIO(f)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    for page in PDFPage.get_pages(fp,\n",
    "                                  pagenos,\n",
    "                                  maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going the apply the function to each url that we extracted from the `Group103.pdf` file, saving the text content in another similar dictionary called `contents_dic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_dic = {}\n",
    "for name,url in links_dic.items():\n",
    "    #save the content of the pdf in the url as a dictionary value\n",
    "    contents_dic[name] = pdf_from_url_to_txt(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algorithms for Non-negative Matrix Factorization\\n\\nAuthored by:\\n\\nH. Sebastian Seung\\n\\nDaniel D. Lee\\n\\nAbstract\\n\\nNon-negative matrix factorization (NMF) has previously been shown\\nto be a useful decomposition for multivariate data. Two diﬀerent multi-\\nplicative algorithms for NMF are analyzed. They diﬀer only slightly in\\nthe multiplicative factor used in the update rules. One algorithm can be\\nshown to minimize the conventional least squares error while the other\\nminimizes the generalized Kullback-Leibler divergence. The monotonic\\nconvergence of both algorithms can be proven using an auxiliary func-\\ntion analogous to that used for proving convergence of the Expectation-\\nMaximization algorithm. The algorithms can also be interpreted as diag-\\nonally rescaled gradient descent, where the rescaling factor is optimally\\nchosen to ensure convergence.\\n\\n1 Paper Body\\n\\nUnsupervised learning algorithms such as principal components analysis and\\nvector quantization can be understood as factorizing a data m'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_dic) # size of the dict\n",
    "# print begining of the first paper\n",
    "contents_dic['PP1861'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Extract the Bodies of the Papers, Authors and Abstract\n",
    "\n",
    "We have created a dictonary `contents_dic` that contains as keys the IDs of the files and as values strings that represent the whole text extracted from the PDF documents. Now, with help from the `re` library we will identify the title, authors, abstract and bodies and break each part into a specific dictonary. This is specially useful for the next sections,  that will first focus exclusively on the bodies of papers and after look at the other sections. <br>\n",
    "These are the regex patterns that we have developed: <br>\n",
    "\n",
    "`pattern_title = r'(.+)?Authored by'` <br>\n",
    "`pattern_author = r'Authored by:(.+)?Abstract'` <br>\n",
    "`pattern_abstract = r'Abstract(.+)?1 Paper Body'` <br>\n",
    "`pattern_body = r'Paper Body(.+)?2 References'`<br>\n",
    "\n",
    "We want to capture a sequence of strings, which means that when the *title* ends, *author* begins. When *author* ends, *abstract* begins. When *abstract* ends, body begins. To find the `title`, we extract everything, with a lazy strategy, until we identify *Authored by*. To find the `authors`, we extract everything, with a lazy strategy, starting from *Authored by:* until we identify *Abstract*. This trend continues until we find *2 References*, extracting the paper body.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating regex patterns to identify title, authors, abstract and body\n",
    "pattern_title, pattern_author, pattern_abstract, pattern_body =\\\n",
    "r'(.+)?Authored by', r'Authored by:(.+?)Abstract', r'Abstract(.+?)1 Paper Body', r'Paper Body(.+?)2 References'\n",
    "\n",
    "# creating empty dictonaries for each part of the papers\n",
    "dict_paper_title, dict_paper_author, dict_paper_abstract, dict_paper_body  = {}, {}, {}, {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predictive Matrix-Variate t Models\\n\\nAuthored by:\\n\\nKai Yu\\n\\nShenghuo Zhu\\nYihong Gong\\n\\nAbstract\\n\\nIt is becoming increasingly important to learn from a partially-observed\\nrandom matrix and predict its missing elements. We assume that the en-\\ntire matrix is a single sample drawn from a matrix-variate t distribution\\nand suggest a matrix-variate t model (MVTM) to predict those missing\\nelements. We show that MVTM generalizes a range of known probabilistic\\nmodels, and automatically performs model selection to encourage sparse\\npredictive models. Due to the non-conjugacy of its prior, it is diﬃcult to\\nmake predictions by computing the mode or mean of the posterior distri-\\nbution. We suggest an optimization method that sequentially minimizes\\na convex upper-bound of the log-likelihood, which is very eﬃcient and\\nscalable. The experiments on a toy data and EachMovie dataset show a\\ngood predictive accuracy of the model.\\n\\n1 Paper Body\\n\\nMatrix analysis techniques, e.g., singular value decomposition (SVD'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_dic['PP3203'][0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the recently created patterns we will identify each part of every paper and populate the new dictonaries.\n",
    "However, we will also do some preprocess cleaning in each of the strings that we find in order to make to next steps with the proper text. These will be done by a function called `clean()`, which does the following corrections:  \n",
    "- Strip strings removing spaces at the beggining and the end\n",
    "- Remove the '-\\n' pattern, usually between a not compound word in a line break. \n",
    "- Replace the page breaks **('\\x0c')** by spaces. To understand it (**next sheet of paper**), we head help from [enigma website](https://www.enigma.com/blog/the-secret-world-of-newline-characters), Yang Yang (2018).\n",
    "- Replace the pattern/number in the bottom of the pages, using the regex `pattern_pagebreak`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_pagebreak = r'\\n\\n(\\d+)?\\n\\n'\n",
    "\n",
    "def clean(item):\n",
    "    item = re.sub(pattern_pagebreak, ' ', item, re.MULTILINE|re.DOTALL)\n",
    "    item = item.replace('-\\n', '').replace('\\x0c', ' ').strip()\n",
    "    return item\n",
    "\n",
    "for id, content in contents_dic.items():\n",
    "\n",
    "    # populating dictionaries with elements, after striping, \n",
    "    # removing '-\\n', replacing '\\n' by ' ' and '\\x0c', that means pagebreak by ' '\n",
    "    dict_paper_title[id] = clean(re.search(pattern_title, content,  re.MULTILINE|re.DOTALL).group(1))\n",
    "    dict_paper_author[id] = clean(re.search(pattern_author, content, re.MULTILINE|re.DOTALL).group(1))\n",
    "    dict_paper_abstract[id] = clean(re.search(pattern_abstract, content,  re.MULTILINE|re.DOTALL).group(1))\n",
    "    dict_paper_body[id] = clean(re.search(pattern_body, content,  re.MULTILINE|re.DOTALL).group(1))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper Title: Algorithms for Non-negative Matrix Factorization \n",
      "\n",
      "Paper Author: H. Sebastian Seung\n",
      "\n",
      "Daniel D. Lee \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing items of first paper to make sure process worked\n",
    "print('Paper Title:', dict_paper_title['PP1861'], '\\n')\n",
    "print('Paper Author:', dict_paper_author['PP1861'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Papers: 200\n",
      "Length of First Article Body: 13291\n"
     ]
    }
   ],
   "source": [
    "print('Number of Papers:', len(dict_paper_body))\n",
    "print('Length of First Article Body:', len(dict_paper_body['PP1861']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Sparse Feature Generation \n",
    "\n",
    "The next tasks aims to generate numerical representations of the each paper body with the relevant words/collocations that can be found on it. However, first we must apply some preprocess tasks following the proposed guidelines. \n",
    "\n",
    "A challenge faced in this assignment was to decide the order to follow of the tasks, as it may impact deeply the final result. It is described in the following every step and the rationale behind this order: \n",
    "\n",
    "1. **Normalization/segmentation**: The first step is to lowercase all words that are in the beggining of a sentence. This step is necessary otherwise in the next ones we would classify differently words such as 'House' and 'house, but we want them to be the same. \n",
    "2. **Tokenization**: We will transform every word in a token (item of a list). To generate bigrams it's more practical that each word is converted to a token.\n",
    "3. **Unify the 200 most relevant bigrams**: It is crucial that we do this step before the next one, that is removing unwanted tokens. If we don't, we may find connected words that were previosly separed by a stopword, for instance. Something to notice is that we do not want to bigrams that have stopwords or length less than 3, so many bigrams will removed before getting to next step.\n",
    "4. **Remove Stop Words and Small Tokens**: context-dependent and independent stopwords, rare tokens and small tokens. The order among the remotion is not relevant, but it is necessary that we do it before stemming words, the last step.\n",
    "4. **Stemming**: The last step is to stem words, that means, to reduce the remaining words to their root. Since we will transform most words, it is essential that it is after removing unwanted tokens. Stemming is not applyied in bigrams.\n",
    "\n",
    "**Relevant observation**: We will work with only one dictonary throughout part 2, `dict_paper_body`. We will perform many transformations on it, but we have decided not create many different dictonaries after every transformation, because it would be inneficient regarding memory usage and also confusing for documentation. As consequence, it is not possible to run all the code, and sudently come to the middle of the assignment and try to run individually one cell.  \n",
    "\n",
    "## 3.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import RegexpTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Lowercase Normalization\n",
    "Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this)\n",
    "\n",
    "This step must be done before tokenization because we need to use sentence segmentation in order to recognize if the word is in the middle of a sentence or not. We are going to use the Punkt Sentence Tokenizer, as seen in the tutorials.\n",
    "\n",
    "*The NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) was designed to split \n",
    "text into sentences \"by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.” It contains a pre-trained sentence tokenizer for English\".* (NLTK Project, 2019)\n",
    "\n",
    "First we must make a pre-trained English sentence tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_detect = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them, we can tokenize all the 200 articles in our `dict_paper_body`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_paper_body = {k:sentence_detect.tokenize(v) for (k,v) in dict_paper_body.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, we need to lower case every first word in it. To do so, we define the `lower_first_word` function that is appliable to each list inside the `dict_paper_body` dictionary. \n",
    "\n",
    "We overwrite a new dictionary with the output. This dictionary links the paper IDs with their bodies, but now with the tokens that appear in the beggining of sentences normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a list of string as intput and return it with the first element of each string lowercased.\n",
    "def lower_first_word(text_list):\n",
    "    lower_cased = [text[0].lower()+text[1:len(text)] for text in text_list]\n",
    "    return lower_cased\n",
    "# apply the function and transform the list back into strings.\n",
    "dict_paper_body = {k:' '.join(lower_first_word(v)) for (k,v) in dict_paper_body.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Papers: 200\n",
      "Length of First Article Body: 13274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'unsupervised learning algorithms such as principal components analysis and\\nvector quantization can be understood as factorizing a data matrix subject to\\ndiﬀerent constraints. depending upon the constraints utilized, the resulting factors can be shown to have very diﬀerent representational properties. principal\\ncomponents analysis enforces only a weak orthogonality constraint, resulting in\\na very distributed representation that uses cancellations to generate variability [1, 2]. on the other hand, vector quantization uses a hard winnertake-all\\nconstraint that results in clustering the data into mutually exclusive prototypes\\n[3]. we have previously shown that nonnegativity is a useful constraint for\\nmatrix factorization that can learn a parts representation of the data [4, 5]. the nonnegative basis vectors that are learned are used in distributed, yet still\\nsparse combinations to generate expressiveness in the reconstructions [6, 7]. in\\nthis submission, we analyze in detail two numerical '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of Papers:', len(dict_paper_body))\n",
    "print('Length of First Article Body:', len(dict_paper_body['PP1861']))\n",
    "dict_paper_body['PP1861'][0:1000] # body, after lowercase normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Tokenization\n",
    "\n",
    "The word tokenization must use the following regular expression, <b>r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"</b>\n",
    "\n",
    "We are going to use `RegexpTokenizier()` from NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) (NLTK Project, 2019). Again, we are overwritting the `dict_paper_body`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Papers: 200\n",
      "Tokens in First Article Body: 1928\n",
      "['unsupervised', 'learning', 'algorithms', 'such', 'as', 'principal', 'components', 'analysis', 'and', 'vector', 'quantization', 'can', 'be', 'understood', 'as', 'factorizing', 'data', 'matrix', 'subject', 'to', 'diﬀerent', 'constraints', 'depending', 'upon', 'the', 'constraints', 'utilized', 'the', 'resulting', 'factors', 'can', 'be', 'shown', 'to', 'have', 'very', 'diﬀerent', 'representational', 'properties', 'principal', 'components', 'analysis', 'enforces', 'only', 'weak', 'orthogonality', 'constraint', 'resulting', 'in', 'very', 'distributed', 'representation', 'that', 'uses', 'cancellations', 'to', 'generate', 'variability', 'on', 'the', 'other', 'hand', 'vector', 'quantization', 'uses', 'hard', 'winnertake-all', 'constraint', 'that', 'results', 'in', 'clustering', 'the', 'data', 'into', 'mutually', 'exclusive', 'prototypes', 'we', 'have', 'previously', 'shown', 'that', 'nonnegativity', 'is', 'useful', 'constraint', 'for', 'matrix', 'factorization', 'that', 'can', 'learn', 'parts', 'representation', 'of', 'the', 'data', 'the', 'nonnegative']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "\n",
    "# tokenize the texts \n",
    "dict_paper_body = {k:tokenizer.tokenize(v) for (k,v) in dict_paper_body.items()}\n",
    "\n",
    "# display the output to analyse eventual mistakes\n",
    "print('Number of Papers:', len(dict_paper_body))\n",
    "print('Tokens in First Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Bigrams\n",
    "\n",
    "The next task is to generate 200 bigram collocations, based on highest total frequency in the corpus; given the tokenized, context-independent-stop-words-free, lower-cased-when-appropriated dictionary of paper bodies. They should be separated using double underscore (example: \"artifical__intelligence\")\n",
    "\n",
    "The first step is to concatenate all the tokenized bodies using the `chain.from_iterable` function from the package [itertools](https://docs.python.org/2/library/itertools.html) (Python Software Foundation, 2019), as done in the tutorials. The output of the function is a flattened list that contains all the words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(chain.from_iterable(dict_paper_body.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate the 200 bigram collocations. We need to import some functions, like `ngrams` from [NLKT](http://www.nltk.org/api/nltk.html#nltk.util.ngrams) in order to perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(all_words, n = 2)\n",
    "fdbigram = FreqDist(bigrams)\n",
    "top_bigrams = fdbigram.most_common(len(fdbigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 6856),\n",
       " (('in', 'the'), 3706),\n",
       " (('to', 'the'), 2705),\n",
       " (('on', 'the'), 2177),\n",
       " (('can', 'be'), 2119)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most of this bigrams have stopwords in it, and this is not of our interest. So, we will select the 200 bigram collocations that don't have any stopword, by excluding than from our list of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = list(fdbigram)\n",
    "\n",
    "# # importing stopwords\n",
    "stopwords_file = open('stopwords_en.txt')\n",
    "stopwords_list = [i.strip() for i in stopwords_file]\n",
    "stopwords_set = set(stopwords_list)\n",
    "stopwords_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('log', 'log'), 225),\n",
       " (('optimization', 'problem'), 223),\n",
       " (('lower', 'bound'), 188),\n",
       " (('training', 'data'), 183),\n",
       " (('loss', 'function'), 179),\n",
       " (('objective', 'function'), 174),\n",
       " (('upper', 'bound'), 150),\n",
       " (('gradient', 'descent'), 145),\n",
       " (('machine', 'learning'), 142),\n",
       " (('posterior', 'distribution'), 132)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords and bigrams that have a word with length less than 3\n",
    "top_bigrams = [token for token in top_bigrams if token[0][0] not in stopwords_list and token[0][1] not in stopwords_list]\n",
    "top_bigrams = [token for token in top_bigrams if len(token[0][0]) > 2 and len(token[0][1]) > 2]\n",
    "top_200_bigrams = top_bigrams[0:200]\n",
    "top_200_bigrams[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have acheived a bigram that is in line with our goals, with no stopwords or words with length less than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Re-tokenize the paper bodies again.\n",
    "\n",
    "Now, we introduce 200 collcations to the token list. we need to make sure those collocations are not split into two individual words. The tokenizer that you need is <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">MWEtokenizer</a> (NLTK Project, 2019). We can use it to transform the bigrams into one unique string separated by \"__\".\n",
    "\n",
    "First, in order to apply the MWETokenizer function, we must reshape the `top_200_bigrams`, that currently is a list of tuples of tuples. What we need is a list of tuples. For instance:\n",
    "\n",
    "[(('log', 'log'), 331),\n",
    " (('optimization', 'problem'), 225)]\n",
    " \n",
    "Should become:\n",
    "\n",
    "[('log', 'log'),\n",
    " ('optimization', 'problem')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('log', 'log'), ('optimization', 'problem'), ('lower', 'bound'), ('training', 'data'), ('loss', 'function'), ('objective', 'function'), ('upper', 'bound'), ('gradient', 'descent'), ('machine', 'learning'), ('posterior', 'distribution')]\n"
     ]
    }
   ],
   "source": [
    "top_200_list = list(map(lambda tuple: tuple[0],top_200_bigrams))\n",
    "print(top_200_list[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list, we will apply the `MWETokenizer` on the `dict_paper_body`'s bodies to have a dictionary with the original tokens and also the appropriated bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams vocabulary size:  27196\n"
     ]
    }
   ],
   "source": [
    "mwetokenizer = MWETokenizer(top_200_list, separator='__')\n",
    "dict_paper_body =  dict((id, mwetokenizer.tokenize(body)) for id,body in dict_paper_body.items())\n",
    "all_words_colloc = list(chain.from_iterable(dict_paper_body.values()))\n",
    "colloc_voc = list(set(all_words_colloc))\n",
    "print('Bigrams vocabulary size: ',len(colloc_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Article Body: 1905\n",
      "['learning__algorithms', 'such', 'as', 'principal__components', 'analysis', 'and', 'vector', 'quantization', 'can', 'be', 'understood', 'as', 'factorizing', 'data__matrix', 'subject', 'to', 'diﬀerent', 'constraints', 'depending', 'upon', 'the', 'constraints', 'utilized', 'the', 'resulting', 'factors', 'can', 'be', 'shown', 'to', 'have', 'very', 'diﬀerent', 'representational', 'properties', 'principal__components', 'analysis', 'enforces', 'only', 'weak', 'orthogonality', 'constraint', 'resulting', 'in', 'very', 'distributed', 'representation', 'that', 'uses', 'cancellations', 'to', 'generate', 'variability', 'on', 'the', 'other', 'hand', 'vector', 'quantization', 'uses', 'hard', 'winnertake-all', 'constraint', 'that', 'results', 'in', 'clustering', 'the', 'data', 'into', 'mutually', 'exclusive', 'prototypes', 'we', 'have', 'previously', 'shown', 'that', 'nonnegativity', 'is', 'useful', 'constraint', 'for', 'matrix__factorization', 'that', 'can', 'learn', 'parts', 'representation', 'of', 'the', 'data', 'the', 'nonnegative', 'basis', 'vectors', 'that', 'are', 'learned']\n"
     ]
    }
   ],
   "source": [
    "# display the output to analyse eventual mistakes\n",
    "print('Tokens in First Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Clean the Body from Unwanted Tokens \n",
    "\n",
    "\n",
    "### 3.5.1. Remove Context Independent Stop Words\n",
    "The context-independent stop words list (i.e, stopwords_en.txt) that was provided is going the be filtered away from our lists of tokens as they as meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = open('stopwords_en.txt')\n",
    "stopwords_list = [i.strip() for i in stopwords_file]\n",
    "stopwords_set = set(stopwords_list)\n",
    "# we check if the lowercased token is in the stopwords_list because we still have some \n",
    "# incorrectly upercase \"The\", \"Therefore\", etc\n",
    "dict_paper_body = {k:[token for token in v if token.lower() not in stopwords_list] for (k,v) in dict_paper_body.items()}\n",
    "stopwords_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Filtered Article Body: 973\n",
      "['unsupervised', 'learning__algorithms', 'principal__components', 'analysis', 'vector', 'quantization', 'understood', 'factorizing', 'data__matrix', 'subject', 'diﬀerent', 'constraints', 'depending', 'constraints', 'utilized', 'resulting', 'factors', 'shown', 'diﬀerent', 'representational', 'properties', 'principal__components', 'analysis', 'enforces', 'weak', 'orthogonality', 'constraint', 'resulting', 'distributed', 'representation', 'cancellations', 'generate', 'variability', 'hand', 'vector', 'quantization', 'hard', 'winnertake-all', 'constraint', 'results', 'clustering', 'data', 'mutually', 'exclusive', 'prototypes', 'previously', 'shown', 'nonnegativity', 'constraint', 'matrix__factorization', 'learn', 'parts', 'representation', 'data', 'nonnegative', 'basis', 'vectors', 'learned', 'distributed', 'sparse', 'combinations', 'generate', 'expressiveness', 'reconstructions', 'submission', 'analyze', 'detail', 'numerical', 'algorithms', 'learning', 'optimal', 'nonnegative', 'factors', 'data', 'Non-negative', 'matrix__factorization', 'formally', 'algorithms', 'solving', 'problem', 'Non-negative', 'matrix__factorization', 'NMF', 'non-negative', 'matrix', 'non-negative', 'matrix', 'factors', 'Wand', 'VWH', 'NMF', 'applied', 'statistical', 'analysis', 'multivariate', 'data', 'manner', 'set', 'multivariate', 'dimensional']\n"
     ]
    }
   ],
   "source": [
    "# analyse filtered_tokens\n",
    "print('Tokens in First Filtered Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. Remove Context Dependent Words\n",
    "\n",
    "Context-dependent (with the threshold set to %95) stop words must be removed from the vocab. The following [article](http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html) (Cambridge University Press, 2009) was of big help to overcome this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2 = list(chain.from_iterable([set(value) for value in dict_paper_body.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "common_tokens = fd_2.most_common(25)\n",
    "\n",
    "content_dependent = [tuple[0] for tuple in common_tokens if (tuple[1]>=190)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the context dependent stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'set', 'number', 'rst']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will filter those words from all our paper bodies in `dict_paper_body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Filtered Article Body: 961\n",
      "['unsupervised', 'learning__algorithms', 'principal__components', 'analysis', 'vector', 'quantization', 'understood', 'factorizing', 'data__matrix', 'subject', 'diﬀerent', 'constraints', 'depending', 'constraints', 'utilized', 'resulting', 'factors', 'shown', 'diﬀerent', 'representational', 'properties', 'principal__components', 'analysis', 'enforces', 'weak', 'orthogonality', 'constraint', 'resulting', 'distributed', 'representation', 'cancellations', 'generate', 'variability', 'hand', 'vector', 'quantization', 'hard', 'winnertake-all', 'constraint', 'clustering', 'data', 'mutually', 'exclusive', 'prototypes', 'previously', 'shown', 'nonnegativity', 'constraint', 'matrix__factorization', 'learn', 'parts', 'representation', 'data', 'nonnegative', 'basis', 'vectors', 'learned', 'distributed', 'sparse', 'combinations', 'generate', 'expressiveness', 'reconstructions', 'submission', 'analyze', 'detail', 'numerical', 'algorithms', 'learning', 'optimal', 'nonnegative', 'factors', 'data', 'Non-negative', 'matrix__factorization', 'formally', 'algorithms', 'solving', 'problem', 'Non-negative', 'matrix__factorization', 'NMF', 'non-negative', 'matrix', 'non-negative', 'matrix', 'factors', 'Wand', 'VWH', 'NMF', 'applied', 'statistical', 'analysis', 'multivariate', 'data', 'manner', 'multivariate', 'dimensional', 'data', 'vectors']\n"
     ]
    }
   ],
   "source": [
    "dict_paper_body = dict((id, [token for token in body if token not in content_dependent]) for id,body in dict_paper_body.items())\n",
    "# filtered_tokens\n",
    "print('Tokens in First Filtered Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3. Removing Tokens with Length Smaller than 3\n",
    "Tokens with the length less than 3 should be removed from the vocab. This must be done before finding the collocations otherwise they would be misidentified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Filtered Article Body: 841\n",
      "['unsupervised', 'learning__algorithms', 'principal__components', 'analysis', 'vector', 'quantization', 'understood', 'factorizing', 'data__matrix', 'subject', 'diﬀerent', 'constraints', 'depending', 'constraints', 'utilized', 'resulting', 'factors', 'shown', 'diﬀerent', 'representational', 'properties', 'principal__components', 'analysis', 'enforces', 'weak', 'orthogonality', 'constraint', 'resulting', 'distributed', 'representation', 'cancellations', 'generate', 'variability', 'hand', 'vector', 'quantization', 'hard', 'winnertake-all', 'constraint', 'clustering', 'data', 'mutually', 'exclusive', 'prototypes', 'previously', 'shown', 'nonnegativity', 'constraint', 'matrix__factorization', 'learn', 'parts', 'representation', 'data', 'nonnegative', 'basis', 'vectors', 'learned', 'distributed', 'sparse', 'combinations', 'generate', 'expressiveness', 'reconstructions', 'submission', 'analyze', 'detail', 'numerical', 'algorithms', 'learning', 'optimal', 'nonnegative', 'factors', 'data', 'Non-negative', 'matrix__factorization', 'formally', 'algorithms', 'solving', 'problem', 'Non-negative', 'matrix__factorization', 'NMF', 'non-negative', 'matrix', 'non-negative', 'matrix', 'factors', 'Wand', 'VWH', 'NMF', 'applied', 'statistical', 'analysis', 'multivariate', 'data', 'manner', 'multivariate', 'dimensional', 'data', 'vectors']\n"
     ]
    }
   ],
   "source": [
    "dict_paper_body = {k:[token for token in v if len(token) >=3] for (k,v) in dict_paper_body.items()}\n",
    "# checking outpup\n",
    "print('Tokens in First Filtered Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4. Remove Rare Words\n",
    "Rare tokens (with the threshold set to 3%) must be removed from the vocabulary. Let's first find out who are those `rare_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare Tokens: 21957\n",
      "['abhahb', 'Vi', 'nonnegativity', 'ilt', 'Proofs', 'Kaufman', 'itt', 'Wh', 'tomography', 'Frobenius-Perron']\n"
     ]
    }
   ],
   "source": [
    "rare_tokens = [key for key,value in fd_2.items() if (value<6)]\n",
    "# checking outpup\n",
    "print('Rare Tokens:', len(rare_tokens))\n",
    "print(rare_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing them from our bodies vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Filtered Article Body: 654\n",
      "['unsupervised', 'learning__algorithms', 'principal__components', 'analysis', 'vector', 'understood', 'data__matrix', 'subject', 'diﬀerent', 'constraints', 'depending', 'constraints', 'utilized', 'resulting', 'factors', 'shown', 'diﬀerent', 'representational', 'properties', 'principal__components', 'analysis', 'enforces', 'weak', 'constraint', 'resulting', 'distributed', 'representation', 'generate', 'variability', 'hand', 'vector', 'hard', 'constraint', 'clustering', 'data', 'mutually', 'exclusive', 'previously', 'shown', 'constraint', 'matrix__factorization', 'learn', 'parts', 'representation', 'data', 'nonnegative', 'basis', 'vectors', 'learned', 'distributed', 'sparse', 'combinations', 'generate', 'analyze', 'detail', 'numerical', 'algorithms', 'learning', 'optimal', 'nonnegative', 'factors', 'data', 'matrix__factorization', 'formally', 'algorithms', 'solving', 'problem', 'matrix__factorization', 'non-negative', 'matrix', 'non-negative', 'matrix', 'factors', 'applied', 'statistical', 'analysis', 'multivariate', 'data', 'manner', 'multivariate', 'dimensional', 'data', 'vectors', 'vectors', 'columns', 'matrix', 'examples', 'data__set', 'matrix', 'approximately', 'factorized', 'matrix', 'matrix', 'chosen', 'smaller', 'smaller', 'original', 'matrix', 'compressed', 'version']\n"
     ]
    }
   ],
   "source": [
    "dict_paper_body = {id: [token for token in body if token not in rare_tokens] for id,body in dict_paper_body.items()}\n",
    "# checking outpup\n",
    "print('Tokens in First Filtered Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Stemming\n",
    "Unigram tokens should be stemmed using the Porter stemmer. (be careful that stemming performs lower casing by default)\n",
    "Porter Stemming Algorithm is the one of the most common stemming algorithms.\n",
    "It makes use of a series of heuristic replacement rules.\n",
    "\n",
    "According to [Stemming and lemmatization](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) (2009), it is *\"The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `stemming_lowercase()` function which applies the `stemmer` on all tokens, except the ones with upercase characters (alleged proper noums) and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_lowercase(token):\n",
    "    # we only do the stemming if it's not a proper noun or a bigram\n",
    "    if (token.lower() == token) and (\"__\" not in token):\n",
    "        new_token = stemmer.stem(token)\n",
    "    else: \n",
    "        new_token = token\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the above defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in First Filtered Article Body: 654\n",
      "['unsupervis', 'learning__algorithms', 'principal__components', 'analysi', 'vector', 'understood', 'data__matrix', 'subject', 'diﬀer', 'constraint', 'depend', 'constraint', 'util', 'result', 'factor', 'shown', 'diﬀer', 'represent', 'properti', 'principal__components', 'analysi', 'enforc', 'weak', 'constraint', 'result', 'distribut', 'represent', 'gener', 'variabl', 'hand', 'vector', 'hard', 'constraint', 'cluster', 'data', 'mutual', 'exclus', 'previous', 'shown', 'constraint', 'matrix__factorization', 'learn', 'part', 'represent', 'data', 'nonneg', 'basi', 'vector', 'learn', 'distribut', 'spars', 'combin', 'gener', 'analyz', 'detail', 'numer', 'algorithm', 'learn', 'optim', 'nonneg', 'factor', 'data', 'matrix__factorization', 'formal', 'algorithm', 'solv', 'problem', 'matrix__factorization', 'non-neg', 'matrix', 'non-neg', 'matrix', 'factor', 'appli', 'statist', 'analysi', 'multivari', 'data', 'manner', 'multivari', 'dimension', 'data', 'vector', 'vector', 'column', 'matrix', 'exampl', 'data__set', 'matrix', 'approxim', 'factor', 'matrix', 'matrix', 'chosen', 'smaller', 'smaller', 'origin', 'matrix', 'compress', 'version']\n"
     ]
    }
   ],
   "source": [
    "dict_paper_body = dict((id, list(map(stemming_lowercase,body))) for id,body in dict_paper_body.items())\n",
    "# checking outpup\n",
    "print('Tokens in First Filtered Article Body:', len(dict_paper_body['PP1861']))\n",
    "print(dict_paper_body['PP1861'][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Paper Bodies - Sparse Feature Generation \n",
    "\n",
    "Each group is required to complete the following two tasks:\n",
    "Generate a sparse representation for Paper Bodies (i.e. paper text without Title, Authors, Abstract and References). The sparse representation consists of two files:\n",
    "### 3.7.1. Vocabulary index file\n",
    "We will create a list that contains all unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADMM',\n",
       " 'AFOSR',\n",
       " 'ARO',\n",
       " 'AUC',\n",
       " 'Acc',\n",
       " 'Accuracy',\n",
       " 'Acknowledgements',\n",
       " 'Acknowledgments',\n",
       " 'Action',\n",
       " 'Adam']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(set(list(chain.from_iterable(dict_paper_body.values()))))\n",
    "vocab_list.sort()\n",
    "vocab_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictonary that has an index number for every element in the list of tokens and exporting the dict to the file `Group103_vocab.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {}\n",
    "vocab_index_file = open('Group103_vocab.txt', 'w+', encoding = \"utf-8\" )\n",
    "\n",
    "for i in range(len(vocab_list)):\n",
    "    index_dict[vocab_list[i]] = i \n",
    "    vocab_index_file.write(vocab_list[i] + ':'+ str(i)+ \"\\n\") # exporting the token and index\n",
    "\n",
    "vocab_index_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2. Sparse count vectors file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, we will create the *count_vectors_file*, named `output/count_vectors.txt`.We will iterate over the final dictonary **dict_paper_body**, that holds the final tokenized structure of each paper. We will count the frequency of every token in each paper and input in a string, called **str_key_value** that is exported every time we iterate in one value of the **dict_paper_body**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectors_file = open('output/count_vectors.txt', 'w+', encoding = \"utf-8\" )\n",
    "\n",
    "for key,value in dict_paper_body.items():\n",
    "\n",
    "    str_key_value = ''   # final string that will be exported, the sum of str_value and str_key\n",
    "    str_value = ''       # string that contains count of every word/index\n",
    "    str_key = str(key)   # string that contains the document ID\n",
    "    token_list = []      # support list, used to avoid duplicates in str_value\n",
    "   \n",
    "    dict_word_freq = dict(FreqDist(value))  #dictonary with the frequency of every token \n",
    "\n",
    "    for item in value: # iterate over the list of tokens\n",
    "\n",
    "        token_list.append(item)\n",
    "        if token_list.count(item) < 2: # this if is to avoid duplicates\n",
    "            str_value = str_value + str(index_dict[item]) + ':' + str(dict_word_freq[item]) + ','\n",
    "       \n",
    "    str_value = str_value[0:len(str_value)-1]  # eliminate the last element, an unecessary ',' \n",
    "    str_key_value = str_key + ',' + str_value # unite the paper id with the sequence of value \n",
    "   \n",
    "    count_vectors_file.write(str_key_value + \"\\n\" ) # Exporting the result to a file\n",
    "   \n",
    "count_vectors_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PP7219,903:7,1661:1,471:2,2269:4,1806:27,2567:3,2529:4,1519:1,2372:9,1330:1,1843:6,987:3,777:1,2245:1,1641:1,1430:1,1899:1,640:1,470:14,2338:2,626:5,1840:1,2381:1,1901:10,1863:61,1328:3,1911:13,2476:17,866:2,1856:10,538:8,1063:28,1941:10,703:2,1230:2,1062:20,2575:7,1156:2,1560:11,1616:2,1154:1,1071:1,678:6,1703:2,965:1,1029:4,1177:1,2118:26,2094:19,1487:10,1893:10,1701:4,1747:1,783:2,469:1,2540:1,2535:1,1196:7,2162:3,2000:4,974:24,1072:20,1658:26,2427:70,887:38,765:1,979:10,1755:5,1260:6,1353:6,2428:9,2082:7,1603:26,1313:2,1563:1,945:1,73:1,283:1,324:1,271:1,231:1,37:1,439:1,1507:1,1982:2,1368:4,482:2,1891:6,1574:33,1526:1,36:13,1162:1,1885:2,2240:2,1776:10,1851:2,774:12,1942:1,1070:2,1345:2,1379:1,542:8,937:1,1322:3,219:1,248:1,268:2,236:3,1983:1,2528:2,571:6,919:2,1517:1,1088:7,1910:1,2286:1,2527:5,982:1,1712:5,920:3,910:1,1004:4,806:3,840:2,1884:3,821:1,1445:5,1245:1,1308:4,2206:1,757:10,1951:1,1935:1,2221:8,2035:5,1589:1,1610:3,2263:4,1821:2,1915:9,1001:9,2385:18,2402:4,1376:5,2182:6,1369:3,1044:35,750:7,590:5,2158:2,1828:1,2111:1,1962:3,2460:2,611:7,2454:8,1387:7,516:1,1497:1,1710:14,1319:15,2058:3,1713:1,1206:4,616:1,1468:2,1292:3,723:32,2216:1,2131:3,2194:7,817:2,1855:1,2014:2,968:1,1234:1,2045:2,2333:2,818:3,1772:2,1158:3,926:3,2108:3,553:1,859:1,484:9,1115:8,495:31,2212:8,2148:2,2383:6,2499:9,1894:8,2170:8,2179:8,1754:5,1542:1,2300:4,612:3,2149:1,1287:1,2163:1,2327:1,651:1,727:11,189:9,1421:3,2514:2,871:1,2005:12,2183:3,2348:1,2329:4,1527:2,580:1,1432:1,904:5,914:4,1157:2,1277:2,2187:1,539:7,90:4,313:1,441:2,118:1,319:1,2153:4,801:3,883:4,2032:1,954:1,1138:3,1428:8,1973:1,1660:2,2328:1,860:4,917:5,1659:1,1657:9,1097:1,569:1,1707:3,2577:3,1266:5,1182:6,2013:1,1065:1,2461:1,1053:1,1036:3,1584:6,1503:7,2462:1,753:2,1505:1,1552:2,1471:5,168:1,1341:1,691:11,1622:1,2217:1,862:1,1049:7,1056:1,2257:4,1034:1,1408:1,2450:1,636:1,1122:2,942:4,930:5,1913:2,434:1,1730:4,241:5,2430:4,686:5,2520:20,1637:3,1443:5,2444:3,2099:5,160:1,2526:12,1651:2,1488:1,803:1,1168:3,1092:11,1109:1,2581:1,255:1,1316:1,1853:1,234:1,159:3,766:1,1599:7,2171:9,574:1,1990:1,1040:1,1786:1,1085:1,483:1,1588:1,2258:1,1108:1,732:1,1746:4,2429:6,1303:7,2549:2,1337:1,1285:2,1133:3,1209:2,2176:5,2371:5,2208:2,1811:3,655:2,1383:3,858:1,480:2,957:6,1329:12,583:3,1897:4,1119:3,1654:1,1294:1,1091:1,1953:17,1649:1,943:1,1293:1,679:1,2545:1,1817:3,549:1,494:3,537:1,2135:2,627:2,764:2,1161:1,2134:1,2151:1,894:3,2446:3,1448:5,1102:2,1483:2,668:1,1565:4,1775:4,1367:1,2147:1,1057:3,2510:1,2294:1,842:1,1338:4,2114:1,1045:1,2292:1,1429:1,1380:1,2222:1,830:1,1561:1,1339:1,976:2,2482:1,1630:4,1485:2,1731:1,2467:2,2126:1,2494:2,2321:2,1670:1,2503:1,878:3,1253:6,1997:2,632:1,1046:2,2290:1,2317:1,1351:3,2354:1,1333:4,1205:2,166:1,281:2,995:1,2491:1,2029:1,1397:2,1431:2,2332:1,14:1,510:1,1778:1,1987:1,907:2,1957:4,953:4,197:1,1956:2,376:1,882:1,722:1,843:3,1010:1,127:1,121:1,1576:6,1093:3,2085:1,340:4,615:2,2198:6,9:1,2586:2,1454:1,2375:1,2562:2,352:2,2419:3,1940:5,1917:2,1718:1,997:1,546:6,754:1,1256:1,1833:2,1346:1,1447:3,137:10,1618:1,1976:3,601:1,18:5,365:4,595:2,647:1,1475:2,872:3,1996:1,990:1,1672:1,1219:1,938:1,1459:2,1831:5,2254:6,1919:2,312:3,1295:1,1257:5,347:2,1691:1,2495:3,2210:1,1435:1,1052:1,932:2,1920:1,419:4,2030:5,278:1,762:1,1084:1,2204:3,2365:1,57:1,239:14,2533:1,2532:1,1698:1,1302:1,1279:1,1392:1,1854:1,1000:2,2141:1,714:1,2465:1,831:2,2180:3,2341:1,705:1,656:1,730:1,1153:2,1033:1,1013:1,1296:1,501:1,2506:10,1845:1,2020:1,1082:1,1848:1,2009:1,2387:3,951:2,464:1,787:1,735:1,2489:1,780:1,2413:2,2090:4,1508:3,653:2,1134:2,1335:2,1241:1,1202:1,832:14,1602:3,2584:3,1895:1,383:1,2522:1,1282:2,1281:2,1860:1,214:1,975:1,139:1,1152:2,502:1,1268:1,1464:1,2038:1,185:2,2524:2,1952:1,1708:2,694:2,1273:1,1554:2,5:3,1269:1,2363:1,591:1,1124:1,1208:1,1604:2,2516:1,2398:5,2042:1,1500:1,1875:1,1598:1,380:1,1512:2,728:1,1151:2,548:1,1557:2,1516:1,1614:1,913:1,1221:1,103:1,2227:1,517:1,2297:1,1289:1,2110:1,1971:1,47:1,2039:1,2347:1,755:2,2367:1,1244:1,1187:1,2453:1,1100:1,1095:1,2214:1,1311:1,7:1,530:1,1146:1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysing last paper's sparse representation:\n",
    "str_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistics Generation\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "To complete this second task, we need to perform the following preprocessing steps on\n",
    "the Titles and Abstracts before extracting the required stats:<br>\n",
    "\n",
    "- <b>A.</b> The word tokenization must use the following regular expression, `r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"`\n",
    "- <b>B.</b> The context-independent stop words (i.e, stopwords_en.txt) must be removed\n",
    "- <b>C.</b> For Abstracts, Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this). For Titles, tokens must be all normalised to lowercase.\n",
    "\n",
    "We will also tokenize the Authors in the `Tokenization` step below.\n",
    "\n",
    "## 4.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Lowercase Normalization\n",
    "For Abstracts, Tokens must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. (use sentence segmentation to achieve this). For Titles, tokens must be all normalised to lowercase.\n",
    "\n",
    "### 4.2.1. Abstracts\n",
    "We are going to do the same process with the abstracts that we have done before with the paper bodies. \n",
    "\n",
    "First, we use the the NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) (NLTK Project, 2019) to split \n",
    "the abstracts contained in the dictionary `dict_paper_abstract` into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non-negative matrix factorization (NMF) has previously been shown\\nto be a useful decomposition for multivariate data.',\n",
       " 'Two diﬀerent multiplicative algorithms for NMF are analyzed.',\n",
       " 'They diﬀer only slightly in\\nthe multiplicative factor used in the update rules.',\n",
       " 'One algorithm can be\\nshown to minimize the conventional least squares error while the other\\nminimizes the generalized Kullback-Leibler divergence.',\n",
       " 'The monotonic\\nconvergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm.',\n",
       " 'The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally\\nchosen to ensure convergence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_paper_abstract = {k:sentence_detect.tokenize(v) for (k,v) in dict_paper_abstract.items()}\n",
    "dict_paper_abstract['PP1861']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, we need to lower case every first word in it. To do so, we use the `lower_first_word` function that is appliable to each list inside the `dict_paper_abstract` dictionary. That is the same function we have previously used for the paper bodies and pre-defined back them.\n",
    "\n",
    "The `dict_paper_abstract` dictionary will be overwritten with the output. This dictionary now links the paper IDs with their abstracts, like it did previously, but now with the tokens that appear in the beggining of sentences normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function and transform the list back into strings.\n",
    "dict_paper_abstract = {k:' '.join(lower_first_word(v)) for (k,v) in dict_paper_abstract.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-negative matrix factorization (NMF) has previously been shown\\nto be a useful decomposition for multivariate data. two diﬀerent multiplicative algorithms for NMF are analyzed. they diﬀer only slightly in\\nthe multiplicative factor used in the update rules. one algorithm can be\\nshown to minimize the conventional least squares error while the other\\nminimizes the generalized Kullback-Leibler divergence. the monotonic\\nconvergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. the algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally\\nchosen to ensure convergence.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_paper_abstract['PP1861'] # abstracts, after lowercase normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Titles\n",
    "For Titles, tokens must be all normalised to lowercase. So, we just need to apply the `lower()` function to all values of the `dict_paper_title` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algorithms for non-negative matrix factorization'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_paper_title = {id: title.lower() for (id, title) in dict_paper_title.items()}\n",
    "dict_paper_title['PP1861']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Tokenization\n",
    "The word tokenization must use the following regular expression, r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"\n",
    "Again, we are going to use `RegexpTokenizier()` from NLTK's [Punkt Sentence Tokenizer](http://www.nltk.org/api/nltk.tokenize.html) (NLTK Project, 2019).\n",
    "\n",
    "We need to tokenize the `dict_paper_title`, `dict_paper_abstract` and `dict_paper_author`. The regex expression for abstracts and titles is the same, the one indicated at the assignment guidelines, <b>r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\"</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer for titles and abstracts\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "# tokenize the titles: \n",
    "dict_paper_title = {k:tokenizer.tokenize(v) for (k,v) in dict_paper_title.items()}\n",
    "# tokenize the abstracts: \n",
    "dict_paper_abstract = {k:tokenizer.tokenize(v) for (k,v) in dict_paper_abstract.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for authors it doesn't work. One example: \n",
    "\n",
    "\"H. Sebastian Seung  Daniel D. Lee\" mus be tokenized as [\"H. Sebastian Seung\", \"Daniel D. Lee\"] and not as [\"H., \"Sebastian\", \"Seung\", \"Daniel\", \"D.\", \"Lee\"]\n",
    "\n",
    "In order to perform such task, we need to adapt the regex expression used to tokenize authors names. The pattern is the following:\n",
    "\n",
    "- If there is one single space \" \" between two names in the `dict_paper_author`, it means it's just a space separating first name and last name. So, it should be kept as one token.\n",
    "- If there is more than one space, the second space came from the line break \"\\n\" conversion to \" \". The original line break was the separator between the authors, so, the split separator should be it: 2 space characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the authors: \n",
    "dict_paper_author= {k:list(filter(None,v.split(\"\\n\"))) for (k,v) in dict_paper_author.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Remove context-independent stop words\n",
    "The context-independent stop words (i.e, stopwords_en.txt) must be removed from the titles and abstracts. As we have previouly done it for the paper bodies, we have already the `stopwords_set` based on the words cointained in the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words form titles:\n",
    "dict_paper_title = {k:[token for token in v if token not in stopwords_set] for (k,v) in dict_paper_title.items()}\n",
    "# remove stop words form abstracts:\n",
    "dict_paper_abstract = {k:[token for token in v if token not in stopwords_set] for (k,v) in dict_paper_abstract.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PP1861': ['algorithms', 'non-negative', 'matrix', 'factorization'], 'PP3203': ['predictive', 'matrix-variate', 'models'], 'PP3216': ['bayesian', 'binning', 'beats', 'approximate', 'alternatives', 'estimating', 'peri-stimulus', 'time', 'histograms'], 'PP3252': ['multiple-instance', 'active', 'learning'], 'PP3282': ['variational', 'inference', 'diﬀusion', 'processes'], 'PP3284': ['gaussian', 'process', 'models', 'link', 'analysis', 'transfer', 'learning'], 'PP3295': ['discriminative', 'batch', 'mode', 'active', 'learning'], 'PP3309': ['inﬁnite', 'gamma-poisson', 'feature', 'model'], 'PP3391': ['hebbian', 'learning', 'bayes', 'optimal', 'decisions'], 'PP3408': ['adaptive', 'template', 'matching', 'shift-invariant', 'semi-nmf'], 'PP3435': ['shared', 'segmentation', 'natural', 'scenes', 'dependent', 'pitman-yor', 'processes'], 'PP3439': ['bio-inspired', 'real', 'time', 'sensory', 'map', 'realignment', 'robotic', 'barn', 'owl'], 'PP3460': ['learning', 'consistency', 'inductive', 'functions', 'kernels'], 'PP3490': ['learning', 'semantic', 'correlation', 'alternative', 'gain', 'unlabeled', 'text'], 'PP3494': ['gaussian-process', 'factor', 'analysis', 'low-dimensional', 'single-trial', 'analysis', 'neural', 'population', 'activity'], 'PP3523': ['biasing', 'approximate', 'dynamic', 'programming', 'lower', 'discount', 'factor'], 'PP3534': ['relative', 'margin', 'machines'], 'PP3580': ['mortal', 'multi-armed', 'bandits'], 'PP3596': ['robust', 'regression', 'lasso'], 'PP3608': ['kernelized', 'sorting'], 'PP3621': ['analyzing', 'human', 'feature', 'learning', 'nonparametric', 'bayesian', 'inference'], 'PP3682': ['particle-based', 'variational', 'inference', 'continuous', 'systems'], 'PP3685': ['sharing', 'features', 'dynamical', 'systems', 'beta', 'processes'], 'PP3726': ['extending', 'phase', 'mechanism', 'diﬀerential', 'motion', 'opponency', 'motion', 'pop-out'], 'PP3736': ['fast', 'learning', 'non-i', 'observations'], 'PP3746': ['discrete', 'mdl', 'predicts', 'total', 'variation'], 'PP3764': ['fmri-based', 'inter-subject', 'cortical', 'alignment', 'functional', 'connectivity'], 'PP3782': ['neural', 'implementation', 'hierarchical', 'bayesian', 'inference', 'importance', 'sampling'], 'PP3804': ['multiple', 'incremental', 'decremental', 'learning', 'support', 'vector', 'machines'], 'PP3825': ['solving', 'stochastic', 'games'], 'PP3837': ['optimizing', 'multi-class', 'spatio-spectral', 'filters', 'bayes', 'error', 'estimation', 'eeg', 'classiﬁcation'], 'PP3855': ['graph-based', 'consensus', 'maximization', 'multiple', 'supervised', 'unsupervised', 'models'], 'PP3861': ['constructing', 'topological', 'maps', 'markov', 'random', 'fields', 'loop-closure', 'detection'], 'PP3912': ['categories', 'functional', 'units', 'inﬁnite', 'hierarchical', 'model', 'brain', 'activations'], 'PP3927': ['distributionally', 'robust', 'markov', 'decision', 'processes'], 'PP3935': ['large', 'margin', 'multi-task', 'metric', 'learning'], 'PP3948': ['functional', 'form', 'motion', 'priors', 'human', 'motion', 'perception'], 'PP3954': ['learning', 'kernels', 'radiuses', 'minimum', 'enclosing', 'balls'], 'PP3992': ['nonparametric', 'bayesian', 'policy', 'priors', 'reinforcement', 'learning'], 'PP4012': ['body-anchored', 'priors', 'identifying', 'actions', 'single', 'images'], 'PP4025': ['throttling', 'poisson', 'processes'], 'PP4040': ['generative', 'local', 'metric', 'learning', 'nearest', 'neighbor', 'classiﬁcation'], 'PP4041': ['learning', 'candidate', 'labeling', 'sets'], 'PP4086': ['linear', 'complementarity', 'regularized', 'policy', 'evaluation', 'improvement'], 'PP4106': ['minimum', 'average', 'cost', 'clustering'], 'PP4130': ['implicit', 'encoding', 'prior', 'probabilities', 'optimal', 'neural', 'populations'], 'PP4166': ['parametric', 'bandits', 'generalized', 'linear', 'case'], 'PP4173': ['probabilistic', 'latent', 'variable', 'models', 'distinguishing', 'eﬀect'], 'PP4193': ['multi-view', 'learning', 'word', 'embeddings', 'cca'], 'PP4194': ['hierarchical', 'multitask', 'structured', 'output', 'learning', 'large-scale', 'sequence', 'segmentation'], 'PP4289': ['empirical', 'models', 'spiking', 'neural', 'populations'], 'PP4292': ['clustered', 'multi-task', 'learning', 'alternating', 'structure', 'optimization'], 'PP4301': ['selective', 'prediction', 'financial', 'trends', 'hidden', 'markov', 'models'], 'PP4314': ['extracting', 'speaker-speciﬁc', 'information', 'regularized', 'siamese', 'deep', 'network'], 'PP4318': ['brain', 'separates', 'face', 'recognition', 'object', 'recognition'], 'PP4337': ['large-scale', 'sparse', 'principal', 'component', 'analysis', 'application', 'text', 'data'], 'PP4339': ['agnostic', 'selective', 'classiﬁcation'], 'PP4359': ['beating', 'sgd', 'learning', 'svms', 'sublinear', 'time'], 'PP4404': ['neuronal', 'adaptation', 'sampling-based', 'probabilistic', 'inference', 'perceptual', 'bistability'], 'PP4427': ['active', 'ranking', 'pairwise', 'comparisons'], 'PP4441': ['generalized', 'lasso', 'based', 'approximation', 'sparse', 'coding', 'visual', 'recognition'], 'PP4447': ['exploiting', 'spatial', 'overlap', 'eﬃciently', 'compute', 'appearance', 'distances', 'image', 'windows'], 'PP4451': ['understanding', 'intrinsic', 'memorability', 'images'], 'PP4454': ['high-dimensional', 'regression', 'noisy', 'missing', 'data', 'provable', 'guarantees', 'non-convexity'], 'PP4466': ['humans', 'teach', 'curriculum', 'learning', 'teaching', 'dimension'], 'PP4484': ['structured', 'learning', 'cell', 'tracking'], 'PP4544': ['learning', 'architecture', 'sum-product', 'networks', 'clustering', 'variables'], 'PP4560': ['semi-supervised', 'eigenvectors', 'locally-biased', 'learning'], 'PP4589': ['repulsive', 'mixtures'], 'PP4595': ['waveform', 'driven', 'plasticity', 'bifeo3', 'memristive', 'devices', 'model', 'implementation'], 'PP4622': ['tensor', 'decomposition', 'fast', 'parsing', 'latent-variable', 'pcfgs'], 'PP4657': ['active', 'learning', 'model', 'evidence', 'bayesian', 'quadrature'], 'PP4666': ['online', 'regret', 'bounds', 'undiscounted', 'continuous', 'reinforcement', 'learning'], 'PP4667': ['entropy', 'estimations', 'correlated', 'symmetric', 'stable', 'random', 'projections'], 'PP4679': ['eﬃcient', 'reinforcement', 'learning', 'high', 'dimensional', 'linear', 'quadratic', 'systems'], 'PP4695': ['coding', 'eﬃciency', 'detectability', 'rate', 'uctuations', 'non-poisson', 'neuronal', 'ring'], 'PP4709': ['no-regret', 'algorithms', 'unconstrained', 'online', 'convex', 'optimization'], 'PP4737': ['nonparametric', 'bayesian', 'inverse', 'reinforcement', 'learning', 'multiple', 'reward', 'functions'], 'PP4762': ['tractable', 'objectives', 'robust', 'policy', 'optimization'], 'PP4788': ['joint', 'modeling', 'matrix', 'text', 'latent', 'binary', 'features'], 'PP4813': ['neurally', 'plausible', 'reinforcement', 'learning', 'working', 'memory', 'tasks'], 'PP4816': ['trajectory-based', 'short-sighted', 'probabilistic', 'planning'], 'PP4852': ['link', 'prediction', 'graphs', 'autoregressive', 'features'], 'PP4895': ['learning', 'invariance', 'linear', 'functionals', 'reproducing', 'kernel', 'hilbert', 'space'], 'PP4896': ['learning', 'kernels', 'local', 'rademacher', 'complexity'], 'PP4900': ['non-strongly', 'convex', 'smooth', 'stochastic', 'approximation', 'convergence', 'rate'], 'PP4902': ['information-theoretic', 'lower', 'bounds', 'distributed', 'statistical', 'estimation', 'communication', 'constraints'], 'PP4907': ['relationship', 'binary', 'classiﬁcation', 'bipartite', 'ranking', 'binary', 'class', 'probability', 'estimation'], 'PP4934': ['approximation', 'faster', 'algorithm', 'proximal', 'average'], 'PP4940': ['linear', 'convergence', 'condition', 'number', 'independent', 'access', 'full', 'gradients'], 'PP5010': ['learning', 'prices', 'repeated', 'auctions', 'strategic', 'buyers'], 'PP5033': ['stochastic', 'optimization', 'pca', 'capped', 'msg'], 'PP5049': ['nonparametric', 'multi-group', 'membership', 'model', 'dynamic', 'networks'], 'PP5066': ['learning', 'eﬃcient', 'random', 'maximum', 'posteriori', 'predictors', 'non-decomposable', 'loss', 'functions'], 'PP5074': ['low-rank', 'matrix', 'reconstruction', 'clustering', 'approximate', 'message', 'passing'], 'PP5078': ['latent', 'maximum', 'margin', 'clustering'], 'PP5096': ['distributed'], 'PP5169': ['geometric', 'optimisation', 'positive', 'deﬁnite', 'matrices', 'elliptically', 'contoured', 'distributions'], 'PP5207': ['deep', 'neural', 'networks', 'object', 'detection'], 'PP5229': ['distributed', 'estimation', 'information', 'loss', 'exponential', 'families'], 'PP5237': ['learning', 'fredholm', 'kernels'], 'PP5238': ['scalable', 'kernel', 'methods', 'doubly', 'stochastic', 'gradients'], 'PP5254': ['quantized', 'estimation', 'gaussian', 'sequence', 'models', 'euclidean', 'balls'], 'PP5281': ['deep', 'fragment', 'embeddings', 'bidirectional', 'image', 'sentence', 'mapping'], 'PP5307': ['convex', 'optimization', 'procedure', 'clustering', 'theoretical', 'revisit'], 'PP5328': ['median', 'selection', 'subset', 'aggregation', 'parallel', 'inference'], 'PP5396': ['distributed', 'power-law', 'graph', 'computing', 'theoretical', 'empirical', 'analysis'], 'PP5398': ['orbit', 'regularization'], 'PP5405': ['synaptical', 'story', 'persistent', 'activity', 'graded', 'lifetime', 'neural', 'system'], 'PP5436': ['exploiting', 'easy', 'data', 'online', 'optimization'], 'PP5443': ['diﬀerence', 'convex', 'functions', 'programming', 'reinforcement', 'learning'], 'PP5448': ['reducing', 'rank', 'relational', 'factorization', 'models', 'including', 'observable', 'patterns'], 'PP5463': ['learning', 'optimize', 'information-directed', 'sampling'], 'PP5475': ['provable', 'tensor', 'factorization', 'missing', 'data'], 'PP5478': ['scaling-up', 'importance', 'sampling', 'markov', 'logic', 'networks'], 'PP5479': ['sparse', 'random', 'feature', 'algorithm', 'coordinate', 'descent', 'hilbert', 'space'], 'PP5496': ['convex', 'deep', 'learning', 'normalized', 'kernels'], 'PP5506': ['deconvolution', 'high', 'dimensional', 'mixtures', 'boosting', 'application', 'diﬀusion-weighted', 'mri', 'human', 'brain'], 'PP5520': ['spectral', 'clustering', 'graphs', 'bethe', 'hessian'], 'PP5570': ['sequential', 'monte', 'carlo', 'graphical', 'models'], 'PP5595': ['just-in', 'time', 'learning', 'fast', 'flexible', 'inference'], 'PP5609': ['parallel', 'successive', 'convex', 'approximation', 'nonsmooth', 'nonconvex', 'optimization'], 'PP5628': ['multitask', 'learning', 'meets', 'tensor', 'factorization', 'task', 'imputation', 'convex', 'optimization'], 'PP5630': ['multivariate', 'regression', 'calibration'], 'PP5644': ['object', 'proposals', 'accurate', 'object', 'class', 'detection'], 'PP5661': ['scalable', 'adaptation', 'state', 'complexity', 'nonparametric', 'hidden', 'markov', 'models'], 'PP5721': ['black-box', 'optimization', 'noisy', 'functions', 'unknown', 'smoothness'], 'PP5773': ['deep', 'generative', 'image', 'models', 'laplacian', 'pyramid', 'adversarial', 'networks'], 'PP5774': ['shepard', 'convolutional', 'neural', 'networks'], 'PP5790': ['unlocking', 'neural', 'population', 'non-stationarities', 'hierarchical', 'dynamics', 'models'], 'PP5827': ['sample', 'complexity', 'episodic', 'fixed-horizon', 'reinforcement', 'learning'], 'PP5829': ['honor', 'hybrid', 'optimization', 'non-convex', 'regularized', 'problems'], 'PP5874': ['gaussian', 'process', 'random', 'fields'], 'PP5890': ['gradient-free', 'hamiltonian', 'monte', 'carlo', 'eﬃcient', 'kernel', 'exponential', 'families'], 'PP5898': ['eﬃcient', 'output', 'kernel', 'learning', 'multiple', 'tasks'], 'PP5905': ['approximating', 'sparse', 'pca', 'incomplete', 'data'], 'PP5917': ['sparse', 'linear', 'programming', 'primal', 'dual', 'augmented', 'coordinate', 'descent'], 'PP5925': ['global', 'linear', 'convergence', 'frank-wolfe', 'optimization', 'variants'], 'PP5999': ['fast', 'classiﬁcation', 'rates', 'high-dimensional', 'gaussian', 'generative', 'models'], 'PP6007': ['lifelong', 'learning', 'non-i', 'tasks'], 'PP6012': ['multi-class', 'svms', 'tighter', 'data-dependent', 'generalization', 'bounds', 'algorithms'], 'PP6017': ['sparse', 'low-rank', 'tensor', 'decomposition'], 'PP6034': ['fast', 'rates', 'exp-concave', 'empirical', 'risk', 'minimization'], 'PP6039': ['sequential', 'neural', 'models', 'stochastic', 'layers'], 'PP6041': ['optimal', 'tagging', 'markov', 'chain', 'optimization'], 'PP6090': ['backprop', 'kf', 'learning', 'discriminative', 'deterministic', 'state', 'estimators'], 'PP6098': ['discriminative', 'gaifman', 'models'], 'PP6124': ['achieving', 'budget-optimality', 'adaptive', 'schemes', 'crowdsourcing'], 'PP6142': ['data', 'poisoning', 'attacks', 'factorization-based', 'collaborative', 'filtering'], 'PP6176': ['scaled', 'bregman', 'theorem', 'applications'], 'PP6186': ['unsupervised', 'learning', 'spoken', 'language', 'visual', 'context'], 'PP6196': ['optimal', 'cluster', 'recovery', 'labeled', 'stochastic', 'block', 'model'], 'PP6202': ['hierarchical', 'question-image', 'co-attention', 'visual', 'question', 'answering'], 'PP6210': ['data', 'driven', 'estimation', 'laplace-beltrami', 'operator'], 'PP6249': ['human', 'decision-making', 'limited', 'time'], 'PP6295': ['active', 'memory', 'replace', 'attention'], 'PP6327': ['full-capacity', 'unitary', 'recurrent', 'neural', 'networks'], 'PP6335': ['deep', 'alternative', 'neural', 'network', 'exploring', 'contexts', 'early', 'action', 'recognition'], 'PP6356': ['probabilistic', 'linear', 'multistep', 'methods'], 'PP6406': ['deep', 'admm-net', 'compressive', 'sensing', 'mri'], 'PP6407': ['homotopy', 'smoothing', 'non-smooth', 'problems', 'lower', 'complexity'], 'PP6412': ['recursive', 'teaching', 'dimension', 'vc', 'classes'], 'PP6415': ['online', 'pricing', 'strategic', 'patient', 'buyers'], 'PP6433': ['spatiotemporal', 'residual', 'networks', 'video', 'action', 'recognition'], 'PP6470': ['eﬃcient', 'neural', 'codes', 'metabolic', 'constraints'], 'PP6532': ['tree-structured', 'reinforcement', 'learning', 'sequential', 'object', 'localization'], 'PP6545': ['consistent', 'kernel', 'estimation', 'functions', 'random', 'variables'], 'PP6547': ['reward', 'augmented', 'maximum', 'likelihood', 'neural', 'structured', 'prediction'], 'PP6559': ['sparse', 'support', 'recovery', 'non-smooth', 'loss', 'functions'], 'PP6628': ['inferring', 'generative', 'model', 'structure', 'static', 'analysis'], 'PP6636': ['maskrnn', 'instance', 'level', 'video', 'object', 'segmentation'], 'PP6670': ['recycling', 'privileged', 'learning', 'distribution', 'matching', 'fairness'], 'PP6683': ['minimizing', 'submodular', 'function', 'samples'], 'PP6728': ['flexible', 'statistical', 'inference', 'mechanistic', 'models', 'neural', 'dynamics'], 'PP6736': ['accelerated', 'consensus', 'min-sum', 'splitting'], 'PP6743': ['regularized', 'modal', 'regression', 'applications', 'cognitive', 'impairment', 'prediction'], 'PP6752': ['neuralfdr', 'learning', 'discovery', 'thresholds', 'hypothesis', 'features'], 'PP6764': ['deep', 'dynamic', 'poisson', 'factorization', 'model'], 'PP6768': ['qsgd', 'communication-eﬃcient', 'sgd', 'gradient', 'quantization', 'encoding'], 'PP6803': ['neural', 'program', 'meta-induction'], 'PP6865': ['diﬀerentially', 'private', 'empirical', 'risk', 'minimization', 'revisited', 'faster', 'general'], 'PP6875': ['online', 'dynamic', 'programming'], 'PP6891': ['deep', 'lattice', 'networks', 'partial', 'monotonic', 'functions'], 'PP6918': ['ensemble', 'sampling'], 'PP6935': ['spherical', 'convolutions', 'application', 'molecular', 'modelling'], 'PP6939': ['tractability', 'structured', 'probability', 'spaces'], 'PP6948': ['collecting', 'telemetry', 'data', 'privately'], 'PP6968': ['diving', 'shallows', 'computational', 'perspective', 'large-scale', 'shallow', 'learning'], 'PP6991': ['fast', 'amortized', 'inference', 'neural', 'activity', 'calcium', 'imaging', 'data', 'variational', 'autoencoders'], 'PP6994': ['successor', 'features', 'transfer', 'reinforcement', 'learning'], 'PP7000': ['multi-armed', 'bandits', 'metric', 'movement', 'costs'], 'PP7024': ['decomposition', 'forecast', 'error', 'prediction', 'markets'], 'PP7077': ['fast', 'sample-eﬃcient', 'algorithms', 'structured', 'phase', 'retrieval'], 'PP7106': ['online', 'learning', 'transductive', 'regret'], 'PP7130': ['svd-softmax', 'fast', 'softmax', 'approximation', 'large', 'vocabulary', 'neural', 'networks'], 'PP7164': ['permutation-based', 'causal', 'inference', 'algorithms', 'interventions'], 'PP7182': ['recurrent', 'ladder', 'networks'], 'PP7191': ['gaussian', 'quadrature', 'kernel', 'features'], 'PP7209': ['learned', 'translation', 'contextualized', 'word', 'vectors'], 'PP7219': ['simple', 'scalable', 'predictive', 'uncertainty', 'estimation', 'deep', 'ensembles']}\n"
     ]
    }
   ],
   "source": [
    "print(dict_paper_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Generate a CSV file (stats.csv) containing three columns:\n",
    "<b>a.</b> Top 10 most frequent terms appearing in all Titles\n",
    "\n",
    "<b>b.</b> Top 10 most frequent Authors\n",
    "\n",
    "<b>c.</b> Top 10 most frequent terms appearing in all Abstracts\n",
    "\n",
    "Note: In case of ties in any of the above fields, settle the tie based on alphabetical ascending order. (example: if the author named John appeared as many times as Mark, then John shall be selected over Mark)\n",
    "\n",
    "First, we define a function that returns a list of the top 10 values in a dictionary of tokens, ordered as instructed. The sorting should be first by reverse order of counts and second by ascending order of names. To do so, we used a `lambda` function inside the `key` of the `sort()`, as [explained](https://stackoverflow.com/questions/14466068/sort-a-list-of-tuples-by-second-value-reverse-true-and-then-by-key-reverse-fal) by mgilson (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top10_sorted(dict):\n",
    "    # first, make a flat list with all the tokens in the dictionary values\n",
    "    words_list = list(chain.from_iterable([value for value in dict.values()]))\n",
    "    # get the tokens' distribution\n",
    "    fd_words = FreqDist(words_list)\n",
    "    # list of tuple pairs (token, number_occurences) ordered by number_occurences\n",
    "    top_words = fd_words.most_common()\n",
    "    # sorting the words with the same number of occurences by alphabetical order\n",
    "    top_words.sort(key=lambda x: (-x[1], x[0]))  # -x[1] for reverse order of occurence number, x[0] for alphabetical ordering\n",
    "    # returning top 10\n",
    "    return top_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to apply the above defined `get_top10_sorted()` function to the `dict_paper_title`, `dict_paper_abstract` and `dict_paper_author`, generating lists to be further added as columns of a dataframe.\n",
    "### 4.5.1 Top 10 most frequent terms appearing in all Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_title = get_top10_sorted(dict_paper_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 48),\n",
       " ('neural', 17),\n",
       " ('models', 16),\n",
       " ('optimization', 12),\n",
       " ('networks', 11),\n",
       " ('deep', 10),\n",
       " ('inference', 10),\n",
       " ('data', 9),\n",
       " ('reinforcement', 9),\n",
       " ('estimation', 8)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Top 10 most frequent Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jakob H. Macke', 4),\n",
       " ('Tomer Koren', 4),\n",
       " ('Dale Schuurmans', 3),\n",
       " ('Daniel D. Lee', 3),\n",
       " ('Huan Xu', 3),\n",
       " ('Michael I. Jordan', 3),\n",
       " ('Pradeep K. Ravikumar', 3),\n",
       " ('Remi Munos', 3),\n",
       " ('Alan A. Stocker', 2),\n",
       " ('Alexander T. Ihler', 2)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_author = get_top10_sorted(dict_paper_author)\n",
    "top_author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Top 10 most frequent terms appearing in all Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 191),\n",
       " ('data', 169),\n",
       " ('model', 153),\n",
       " ('algorithm', 140),\n",
       " ('show', 114),\n",
       " ('problem', 109),\n",
       " ('approach', 93),\n",
       " ('models', 86),\n",
       " ('algorithms', 84),\n",
       " ('methods', 84)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_abstract = get_top10_sorted(dict_paper_abstract)\n",
    "top_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Generate the CSV\n",
    "First, we will organize our top 10 words of each section in columns of a dataframe for them write the dataframe to a csv. \n",
    "\n",
    "We create a `zipped_list` with the abstract, title and author lists of words, following the approach [suggested](https://thispointer.com/python-pandas-how-to-convert-lists-to-a-dataframe`) in thispointer.com (Varun, 2018). Them, we use it to generate our `df_statistics` dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>Jakob H. Macke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>neural</td>\n",
       "      <td>Tomer Koren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>models</td>\n",
       "      <td>Dale Schuurmans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>optimization</td>\n",
       "      <td>Daniel D. Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>show</td>\n",
       "      <td>networks</td>\n",
       "      <td>Huan Xu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>problem</td>\n",
       "      <td>deep</td>\n",
       "      <td>Michael I. Jordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>approach</td>\n",
       "      <td>inference</td>\n",
       "      <td>Pradeep K. Ravikumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>models</td>\n",
       "      <td>data</td>\n",
       "      <td>Remi Munos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>reinforcement</td>\n",
       "      <td>Alan A. Stocker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>methods</td>\n",
       "      <td>estimation</td>\n",
       "      <td>Alexander T. Ihler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_terms_in_abstracts top10_terms_in_titles         top10_authors\n",
       "0                 learning              learning        Jakob H. Macke\n",
       "1                     data                neural           Tomer Koren\n",
       "2                    model                models       Dale Schuurmans\n",
       "3                algorithm          optimization         Daniel D. Lee\n",
       "4                     show              networks               Huan Xu\n",
       "5                  problem                  deep     Michael I. Jordan\n",
       "6                 approach             inference  Pradeep K. Ravikumar\n",
       "7                   models                  data            Remi Munos\n",
       "8               algorithms         reinforcement       Alan A. Stocker\n",
       "9                  methods            estimation    Alexander T. Ihler"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_list =  list(zip([i[0] for i in top_abstract], [i[0] for i in top_title], [i[0] for i in top_author]))\n",
    "df_statistics = pd.DataFrame(zipped_list, columns = ['top10_terms_in_abstracts' , 'top10_terms_in_titles', 'top10_authors'])\n",
    "df_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `pandas` function `to_csv()` [function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv) (The `pandas` Project, 2019) to write our final CSV file with all the required statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_statistics.to_csv(\"output/stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "\n",
    "This assessment measured the understanding of text file preprocessing techniques, file handling, PDF data extraction, as well as generating numerical modelling - for instance, sparce representation - of texts in the Python programming language. The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "- **Download PDF files from URLs and convert them in string**. Using the libraries such as pdfminer and request, we have have donwloaded 200 PDF's files and converted than to text that populated dictonaries, with the structure `Paper_ID: Extracted_text`. However, after this step was completed, with use of regex, we broke the dictonary into different 4 parts (other dictonaries). One for authors, other for title, abstract and body. This was useful, specially because the fisrt part of the assignment was exclusevely related to the bodies of the papers.\n",
    "- **Text Pre-Processing**. A relevant number of tasks were done in order to achieve the proper format to proceed to the modelling tasks. The biggest challenge faced in this part of the assigment, was not any of the tasks, but to understand wich was the correct order to deploy each of the steps. We have decided to unite all activites related to words/tokens removal together, after the processes of segmentation, tokenization and generating bigrams. The last step was to stem the tokens. \n",
    "- **Generating Numerical Representations**. Three different documents were generated in this assignment: The first contains a numerical representation for each token, found in the corpus. The second was a sparce representation, where the count of each index in each paper can be found. In the third, ranking statistics about the corpus were generated. The Freqdist tool, from the NLTK library was specially useful to overcome all difficulties faced regarding generating numerical forms from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. References\n",
    "- Haseeb M. (2018, November 17). *Parsing a PDF via URL with Python using pdfminer* [Response to]. Retrieved from https://stackoverflow.com/questions/22800100/parsing-a-pdf-via-url-with-python-using-pdfminer\n",
    "\n",
    "- Haseeb M. (2018, October 14). *Pdfminer python 3.5* [Response to]. Retrieved from https://stackoverflow.com/questions/39854841/pdfminer-python-3-5?fbclid=IwAR0btjjjuzFet2zfp4Rhle3IG-ZOKP0iAAeToU7ewI7ly1-BLKcrS0MGDB8\n",
    "\n",
    "- Aryan A. (2017, April 17). *Downloading Files from URLs in Python*. Retrieved from https://www.codementor.io/aviaryan/downloading-files-from-urls-in-python-77q3bs0un\n",
    "\n",
    "- Guru99. (Accessed on 2019, September 09). *Python Regex: re.match(), re.search(), re.findall() with Example*. Retrieved from\n",
    "https://www.guru99.com/python-regular-expressions-complete-tutorial.html\n",
    "\n",
    "- Yang Yang (June 19, 2018). *The Secret World of Newline Characters*. Retrieved from\n",
    "https://www.enigma.com/blog/the-secret-world-of-newline-characters \n",
    "\n",
    "- Cambridge University Press (April 07, 2009). *Dropping common terms: stop words*. Retrieved from\n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html\n",
    "\n",
    "- Cambridge University Press (April 07, 2009). *Stemming and lemmatization*. Retrieved from\n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "- World International Property Organization (2016). *Python Pandas : How to convert lists to a dataframe*. Retrieved from http://www.wipo.int/export/sites/www/classifications/ipc/en/guide/guide_ipc.pdf\n",
    "\n",
    "- Varun (2018, September 25). *NLTK 3.4.5 documentation: nltk.tokenize module*. Retrieved from https://thispointer.com/python-pandas-how-to-convert-lists-to-a-dataframe/\n",
    "\n",
    "- Python Software Foundation. (2019). itertools *Functions creating iterators for efficient looping*. Retrieved from https://docs.python.org/2/library/itertools.html\n",
    "\n",
    "- NLTK Project. (2019). *NLTK 3.4.5 documentation: nltk.tokenize module*. Retrieved from http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "- The pandas Project. (2019). *pandas 0.25.1 documentation: pandas.DataFrame.to_csv*. Retrieved from http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv\n",
    "\n",
    "- mgilson (2013, January 22). *Sort a list of tuples by second value, reverse=True and then by key, reverse=False* [Response to]. Retrieved from https://stackoverflow.com/questions/14466068/sort-a-list-of-tuples-by-second-value-reverse-true-and-then-by-key-reverse-fal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
